--- Running Sanity Checks with Dummy Data ---

--- Part 1: Tokenization START ---

--- Part 1.1: Vocabulary START ---
Building vocabulary from dummy_train.txt...
Found 25 unique raw tokens.
Truncating vocabulary to 11 most common words + 4 special tokens.
Final vocabulary size: 15
[Check 1.1_1] Vocab size <= 15?
PASS. Size is 15.

[Check 1.1_2] Special symbols exist and are correct?
PASS. Special tokens mapped: BOS:0, EOS:1, UNK:2, PAD:3.

[Check 1.1_3] Frequent/Rare words included/excluded?
PASS. Frequent words ('the', 'and', '.') are in vocab.
PASS. Rare words ('cuboidal', 'epiglottis') are not in vocab.

[Check 1.1_4] Round-trip mapping?
PASS. In-vocab: 'the' -> 4 -> 'the'
PASS. Out-of-vocab: 'cuboidal' -> 2 -> '<UNK>'

--- Part 1.1 Checks Passed ---


--- Part 1.2: Tokenizer Functionality START ---
Input texts: ['This is a test.', 'Another test.']

[Check 1.2_1] Tokenizer output:
Type: <class 'transformers.tokenization_utils_base.BatchEncoding'>
Keys: KeysView({'input_ids': tensor([[ 0,  2,  2,  9,  7,  5,  1],
        [ 0, 11,  7,  5,  1,  3,  3]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 0, 0]])})

'input_ids':
tensor([[ 0,  2,  2,  9,  7,  5,  1],
        [ 0, 11,  7,  5,  1,  3,  3]])

'attention_mask':
tensor([[1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 0, 0]])

PASS. Output shape is torch.Size([2, 7]).
PASS. Padding and attention mask are correct.
PASS. Truncation to model_max_length works.

[Check 1.2_2] Save and Load:
PASS. Loaded tokenizer has same vocab size and pad ID.

--- Part 1.2 Checks Passed ---

--- Part 1 Checks Passed ---


--- Part 2: Loading the text files and creating batches ---


--- Part 2.1: Dataset Loading START ---
Loading datasets...
Generating train split: 8 examples [00:00, 8004.40 examples/s]
Generating val split: 3 examples [00:00, 4746.48 examples/s]
Original dataset sizes: Train=8, Val=3
Filtering empty lines...
Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 2005.76 examples/s]
Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1039.57 examples/s]

[Check 2.1_1] Filtered dataset sizes:
Train: 7
Val: 2

[Check 2.1_2] Subsetting dataset (optional step):
Subset sizes: Train=2, Val=2
PASS. Subsetting works.

--- Part 2.1 Checks Passed ---


--- Part 2.2: DataLoader START ---
Creating DataLoader with batch_size=2...

[Check 2.2_1] Fetching first batch:
{'input_ids': tensor([[ 0,  4, 12, 13,  8, 14,  2,  4,  2,  1],
        [ 0,  9,  7,  2, 10,  4,  6,  6,  5,  1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}

PASS. First batch fetched successfully and is a BatchEncoding with correct tensors.

--- Part 2.2 Checks Passed ---

--- Part 3: RNN Model START ---
Initializing A1RNNModel with RNN type: LSTM
Model output shape: torch.Size([2, 6, 15])
Traceback (most recent call last):
  File "/data/users/yinghao/Projects/a1/A1_test.py", line 246, in <module>
    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
              ^^
NameError: name 'nn' is not defined
(dat450_venv) yinghao@minerva:~/Projects/a1$ /data/courses/2025_dat450_dit247/venvs/dat450_venv/bin/python /data/users/yinghao/Projects/a1/A1_test.py


--- Running Sanity Checks with Dummy Data ---

--- Part 1: Tokenization START ---

--- Part 1.1: Vocabulary START ---
Building vocabulary from dummy_train.txt...
Found 25 unique raw tokens.
Truncating vocabulary to 11 most common words + 4 special tokens.
Final vocabulary size: 15
[Check 1.1_1] Vocab size <= 15?
PASS. Size is 15.

[Check 1.1_2] Special symbols exist and are correct?
PASS. Special tokens mapped: BOS:0, EOS:1, UNK:2, PAD:3.

[Check 1.1_3] Frequent/Rare words included/excluded?
PASS. Frequent words ('the', 'and', '.') are in vocab.
PASS. Rare words ('cuboidal', 'epiglottis') are not in vocab.

[Check 1.1_4] Round-trip mapping?
PASS. In-vocab: 'the' -> 4 -> 'the'
PASS. Out-of-vocab: 'cuboidal' -> 2 -> '<UNK>'

--- Part 1.1 Checks Passed ---


--- Part 1.2: Tokenizer Functionality START ---
Input texts: ['This is a test.', 'Another test.']

[Check 1.2_1] Tokenizer output:
Type: <class 'transformers.tokenization_utils_base.BatchEncoding'>
Keys: KeysView({'input_ids': tensor([[ 0,  2,  2,  9,  7,  5,  1],
        [ 0, 11,  7,  5,  1,  3,  3]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 0, 0]])})

'input_ids':
tensor([[ 0,  2,  2,  9,  7,  5,  1],
        [ 0, 11,  7,  5,  1,  3,  3]])

'attention_mask':
tensor([[1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 0, 0]])

PASS. Output shape is torch.Size([2, 7]).
PASS. Padding and attention mask are correct.
PASS. Truncation to model_max_length works.

[Check 1.2_2] Save and Load:
PASS. Loaded tokenizer has same vocab size and pad ID.

--- Part 1.2 Checks Passed ---

--- Part 1 Checks Passed ---


--- Part 2: Loading the text files and creating batches ---


--- Part 2.1: Dataset Loading START ---
Loading datasets...
Generating train split: 8 examples [00:00, 7099.96 examples/s]
Generating val split: 3 examples [00:00, 4542.57 examples/s]
Original dataset sizes: Train=8, Val=3
Filtering empty lines...
Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 2887.89 examples/s]
Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1600.88 examples/s]

[Check 2.1_1] Filtered dataset sizes:
Train: 7
Val: 2

[Check 2.1_2] Subsetting dataset (optional step):
Subset sizes: Train=2, Val=2
PASS. Subsetting works.

--- Part 2.1 Checks Passed ---


--- Part 2.2: DataLoader START ---
Creating DataLoader with batch_size=2...

[Check 2.2_1] Fetching first batch:
{'input_ids': tensor([[ 0,  4, 12, 13,  8, 14,  2,  4,  2,  1],
        [ 0,  9,  7,  2, 10,  4,  6,  6,  5,  1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}

PASS. First batch fetched successfully and is a BatchEncoding with correct tensors.

--- Part 2.2 Checks Passed ---

--- Part 3: RNN Model START ---
Initializing A1RNNModel with RNN type: LSTM
Model output shape: torch.Size([2, 6, 15])
Loss: 2.6259407997131348

--- Part 3 PASS. Model forward and loss computation work. ---

--- Part 5: Evaluation and analysis START ---
Load pre-trained model...
Initializing A1RNNModel with RNN type: LSTM
Model and tokenizer loaded successfully on device: cpu

[Check 5.1] Predicting the next word...

Prompt: She lives in San
Greedy next word: diego
Top-5 candidates:
  1. diego (p=0.5253)
  2. francisco (p=0.3784)
  3. suu (p=0.0545)
  4. salvador (p=0.0263)
  5. jose (p=0.0155)

Prompt: The capital of France is
Greedy next word: the
Top-5 candidates:
  1. the (p=0.4724)
  2. a (p=0.2167)
  3. located (p=0.1089)
  4. <UNK> (p=0.1045)
  5. home (p=0.0975)

Prompt: Deep learning models are trained with
Greedy next word: a
Top-5 candidates:
  1. a (p=0.3292)
  2. the (p=0.2798)
  3. <UNK> (p=0.2491)
  4. an (p=0.0784)
  5. more (p=0.0634)

 Predictions saved to /data/users/yinghao/Projects/a1/part5_nextword.txt
predict_next_word() ran successfully.

[Check 5.2] Evaluating perplexity on dummy validation set...
Generating val split: 3 examples [00:00, 3475.94 examples/s]
Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1613.40 examples/s]
Validation Cross-Entropy Loss: 5.6280
Validation Perplexity: 278.10
Perplexity computation successful: 278.10

[Check 5.3_1] Inspecting nearest neighbors...

 Nearest neighbors for 'sweden':
  norway          (cos=0.2674)
  introduction    (cos=0.2403)
  nepal           (cos=0.2321)
  dissipated      (cos=0.2275)
  dorylaeum       (cos=0.2269)

Top neighbors for 'sweden':
  norway          (cos=0.2674)
  introduction    (cos=0.2403)
  nepal           (cos=0.2321)
  dissipated      (cos=0.2275)
  dorylaeum       (cos=0.2269)

 Nearest neighbors for 'king':
  ditta           (cos=0.2691)
  paracrine       (cos=0.2566)
  aden            (cos=0.2436)
  painter         (cos=0.2412)
  dramatist       (cos=0.2404)

Top neighbors for 'king':
  ditta           (cos=0.2691)
  paracrine       (cos=0.2566)
  aden            (cos=0.2436)
  painter         (cos=0.2412)
  dramatist       (cos=0.2404)

 Nearest neighbors for 'city':
  vig             (cos=0.2599)
  dh.98           (cos=0.2486)
  freebase        (cos=0.2454)
  machine         (cos=0.2373)
  müzesi          (cos=0.2311)

Top neighbors for 'city':
  vig             (cos=0.2599)
  dh.98           (cos=0.2486)
  freebase        (cos=0.2454)
  machine         (cos=0.2373)
  müzesi          (cos=0.2311)

 Nearest neighbors for 'woman':
  verb            (cos=0.2562)
  donn            (cos=0.2561)
  prizes          (cos=0.2517)
  saxophonist     (cos=0.2345)
  close-up        (cos=0.2324)

Top neighbors for 'woman':
  verb            (cos=0.2562)
  donn            (cos=0.2561)
  prizes          (cos=0.2517)
  saxophonist     (cos=0.2345)
  close-up        (cos=0.2324)

 Nearest neighbors for 'love':
  oscillations    (cos=0.2521)
  swami           (cos=0.2408)
  sublimation     (cos=0.2371)
  sean            (cos=0.2357)
  professionalism (cos=0.2347)

Top neighbors for 'love':
  oscillations    (cos=0.2521)
  swami           (cos=0.2408)
  sublimation     (cos=0.2371)
  sean            (cos=0.2357)
  professionalism (cos=0.2347)
Nearest neighbor inspection successful.

[Check 5.3_2] PCA visualization of embeddings...
Saved PCA visualization to /data/users/yinghao/Projects/a1/pca_embeddings.png
PCA embedding plot saved successfully.

--- Part 5 Checks Passed ---


Cleaning up dummy files...
Done.